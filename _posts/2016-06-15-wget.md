---
layout: post
title: '[Linux] wget 常見用法'
keywords: wget, download, 下載, linux, 下載網站, 備份
published: true
---

## 內容
- [下載檔案](#downloadFile)
- [備份網頁](#backupPage)
- [下載整個網站](#downloadWebsite)

---

<a name="downloadFile"></a>

## 1.下載檔案

### 下載

普通的下載檔案<br>

```bash
wget http://ipv4.download.thinkbroadband.com/5MB.zip -O 5MB.zip

-O  下載下來的檔名
```

### 續傳

檔案太大的話可以用續傳功能<br>
上次下載到哪裡會繼續下載<br>

```bash
wget -c http://ipv4.download.thinkbroadband.com/5MB.zip

-c  續傳
```

上次下載到 25% 網路就斷了<br>
![](http://imgur.com/RHt0Erp.png)
繼續下載到 58%<br>
![](http://imgur.com/cD9sg5m.png)

### 重試

如果網路不穩時可以設定重試次數

```bash
wget -t 10 http://ipv4.download.thinkbroadband.com/5MB.zip

-t 10   最多重試 10 次
-t 0    不斷重試直到成功為止
```

### 速度上限

也可以設定速度上限

```bash
wget --limit-rate=200k http://ipv4.download.thinkbroadband.com/5MB.zip

--limit-rate    速度上限
```
下載速限 200KB/s

---

<a name="backupPage"></a>

## 2.備份網頁

### 備份 Google 搜尋的網頁

![](http://imgur.com/4sApmZP.png)

```bash
wget -p -k https://www.google.com.tw

-p  下載網頁所需的所有檔案, 如圖片.背景音樂
-k  將相對連結轉為絕對連結, 可離線瀏覽
```

下載完會生出一個像這樣的資料夾<br>
![](http://i.imgur.com/JMjX9kH.png)<br>
進去把 index.html 打開就可以看到下載的網頁了<br>
![](http://i.imgur.com/WkB5CJH.png)<br>
再進到 images 裡面看一下<br>
![](http://imgur.com/u18AwZs.png)<br>
就可以找到 google 的 logo ~ <br>
![](http://imgur.com/thElnfF.png)<br>

---

<a name="downloadWebsite"></a>

## 3.下載整個網站

網站 != 網頁<br>
一個網站可能有很多網頁<br>

### 下載整個"演算法筆記"網站

來看看<a href="http://www.csie.ntnu.edu.tw/~u91029/index.html" target="_blank">演算法筆記</a>這個網站<br>
![](http://imgur.com/Y4iMtPw.png)
這個網站裡面有很多網頁<br>
如果要把整個網站下載下來<br>

```bash
wget -r -p -np -k http://www.csie.ntnu.edu.tw/~u91029/index.html

-r   遞迴下載網站內的所有檔案
-p   下載網頁所需的所有檔案, 如圖片.背景音樂
-np  不要下載父目錄的東西
-k   將相對連結轉為絕對連結, 可離線瀏覽
```

因為整個網站有很多圖片跟文字<br>
所以要有點久<br>
(我自己下載花了六分鐘左右)<br>
<br>
下載完進到裡面找 index.html 就可以離線瀏覽了<br>
![](http://imgur.com/O7ztPzf.png)

---

以上是比較常見的 wget 用法<br>
我個人覺得最好用的是下載網站的功能～<br>
下載之後就可以直接在本機上看<br>
偶爾在更新重新下載一次就可以了<br>

